{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1:06:00 into the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# Create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix1)\n",
    "    # N[ix1, ix2] += 1\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5065, -1.6001,  0.1543, -1.5038,  0.6003,  0.1293,  0.3801,  0.9329,\n",
       "         -0.1734,  1.0973, -0.7667, -0.4664, -1.5550,  0.6321, -0.8354, -1.0189,\n",
       "          0.9538,  1.0722, -0.1760,  1.7314, -0.5848,  0.6770, -0.1518,  0.4151,\n",
       "         -1.7621, -0.8457,  1.6124],\n",
       "        [-0.0786,  0.4371,  1.6178,  0.8398, -1.1630,  0.5914, -0.7581, -0.9462,\n",
       "         -0.8864,  0.0635, -0.2182, -1.1287, -1.6756,  0.8278, -0.3998, -0.3225,\n",
       "          1.6548,  0.9399, -0.0143,  0.5533,  0.1249,  1.9361,  0.8274, -1.9637,\n",
       "         -0.0043,  0.2374, -0.3217],\n",
       "        [ 0.0624, -0.8057,  0.7439,  0.0466, -0.3992,  0.2192,  0.1475, -0.5697,\n",
       "         -0.5436,  1.0771, -0.9591,  1.6696, -1.1074, -0.3922, -1.1530,  1.6938,\n",
       "         -0.6793,  1.5330, -0.1629,  0.3364, -0.3267, -0.3821, -1.7494, -1.2651,\n",
       "          0.3915, -0.9126, -0.5953],\n",
       "        [ 0.0624, -0.8057,  0.7439,  0.0466, -0.3992,  0.2192,  0.1475, -0.5697,\n",
       "         -0.5436,  1.0771, -0.9591,  1.6696, -1.1074, -0.3922, -1.1530,  1.6938,\n",
       "         -0.6793,  1.5330, -0.1629,  0.3364, -0.3267, -0.3821, -1.7494, -1.2651,\n",
       "          0.3915, -0.9126, -0.5953],\n",
       "        [-0.9424,  0.3913,  0.3965,  0.1180, -0.1165, -0.3077,  0.8580,  0.5568,\n",
       "          0.3089,  0.0948,  0.1936,  1.3374, -0.1929, -0.1252, -0.7050, -0.0737,\n",
       "          0.2394, -1.3049, -1.2577, -0.6524,  0.8430, -0.9492,  1.1672,  1.6789,\n",
       "          1.2872, -2.9286,  2.2680]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ this is a matrix multiplication operator in pytorch\n",
    "\n",
    "W = torch.randn((27, 27))\n",
    "xenc @ W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0153, 0.0051, 0.0295, 0.0056, 0.0462, 0.0288, 0.0370, 0.0644, 0.0213,\n",
       "         0.0759, 0.0118, 0.0159, 0.0053, 0.0476, 0.0110, 0.0091, 0.0657, 0.0740,\n",
       "         0.0212, 0.1430, 0.0141, 0.0498, 0.0218, 0.0383, 0.0043, 0.0109, 0.1270],\n",
       "        [0.0214, 0.0358, 0.1165, 0.0535, 0.0072, 0.0417, 0.0108, 0.0090, 0.0095,\n",
       "         0.0246, 0.0186, 0.0075, 0.0043, 0.0529, 0.0155, 0.0167, 0.1209, 0.0591,\n",
       "         0.0228, 0.0402, 0.0262, 0.1602, 0.0529, 0.0032, 0.0230, 0.0293, 0.0167],\n",
       "        [0.0296, 0.0124, 0.0585, 0.0291, 0.0187, 0.0346, 0.0322, 0.0157, 0.0161,\n",
       "         0.0817, 0.0107, 0.1477, 0.0092, 0.0188, 0.0088, 0.1513, 0.0141, 0.1288,\n",
       "         0.0236, 0.0389, 0.0201, 0.0190, 0.0048, 0.0078, 0.0411, 0.0112, 0.0153],\n",
       "        [0.0296, 0.0124, 0.0585, 0.0291, 0.0187, 0.0346, 0.0322, 0.0157, 0.0161,\n",
       "         0.0817, 0.0107, 0.1477, 0.0092, 0.0188, 0.0088, 0.1513, 0.0141, 0.1288,\n",
       "         0.0236, 0.0389, 0.0201, 0.0190, 0.0048, 0.0078, 0.0411, 0.0112, 0.0153],\n",
       "        [0.0082, 0.0309, 0.0311, 0.0235, 0.0186, 0.0154, 0.0494, 0.0365, 0.0285,\n",
       "         0.0230, 0.0254, 0.0797, 0.0173, 0.0185, 0.0103, 0.0194, 0.0266, 0.0057,\n",
       "         0.0059, 0.0109, 0.0486, 0.0081, 0.0672, 0.1122, 0.0758, 0.0011, 0.2021]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # equivalent N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY ------------------>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neuros's weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # Input to the network: one-hot encoding \n",
    "logits = xenc @ W # predict log-counts \n",
    "counts = logits.exp() # counts, equivalent to N \n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character \n",
    "# btw: the last 2 lines here are together called a \"softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .. (indexes 0,0)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.06067225709557533\n",
      "log likelihood: -2.8022687435150146\n",
      "negative log likelihood: 2.8022687435150146\n",
      "--------\n",
      "bigram example 2: ee (indexes 5,5)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.028868859633803368\n",
      "log likelihood: -3.5449917316436768\n",
      "negative log likelihood: 3.5449917316436768\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 4: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 5: aa (indexes 1,1)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.008642823435366154\n",
      "log likelihood: -4.751026153564453\n",
      "negative log likelihood: 4.751026153564453\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.669020891189575\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- !!! OPTIMIZATION !!! ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neuros's weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # Input to the network: one-hot encoding \n",
    "logits = xenc @ W # predict log-counts \n",
    "counts = logits.exp() # counts, equivalent to N \n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character \n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6122443675994873\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to the tensor\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Putting it all together -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# Create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    #print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the \"network\"\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.672508716583252\n",
      "3.321807622909546\n",
      "3.1243221759796143\n",
      "3.000220775604248\n",
      "2.91317081451416\n",
      "2.8495664596557617\n",
      "2.8013453483581543\n",
      "2.7633864879608154\n",
      "2.7325382232666016\n",
      "2.7068352699279785\n",
      "2.685014009475708\n",
      "2.6662347316741943\n",
      "2.6499154567718506\n",
      "2.635629415512085\n",
      "2.623049259185791\n",
      "2.611915111541748\n",
      "2.602012872695923\n",
      "2.5931639671325684\n",
      "2.5852203369140625\n",
      "2.5780556201934814\n",
      "2.5715653896331787\n",
      "2.5656604766845703\n",
      "2.5602667331695557\n",
      "2.5553207397460938\n",
      "2.550769090652466\n",
      "2.5465660095214844\n",
      "2.5426740646362305\n",
      "2.5390591621398926\n",
      "2.535693883895874\n",
      "2.5325541496276855\n",
      "2.5296192169189453\n",
      "2.5268704891204834\n",
      "2.5242919921875\n",
      "2.5218701362609863\n",
      "2.5195915699005127\n",
      "2.5174458026885986\n",
      "2.5154218673706055\n",
      "2.5135111808776855\n",
      "2.511704444885254\n",
      "2.5099949836730957\n",
      "2.5083751678466797\n",
      "2.5068390369415283\n",
      "2.505380392074585\n",
      "2.5039939880371094\n",
      "2.5026748180389404\n",
      "2.5014188289642334\n",
      "2.500220775604248\n",
      "2.4990782737731934\n",
      "2.4979865550994873\n",
      "2.496943235397339\n",
      "2.4959447383880615\n",
      "2.4949886798858643\n",
      "2.4940719604492188\n",
      "2.4931929111480713\n",
      "2.4923489093780518\n",
      "2.4915385246276855\n",
      "2.4907586574554443\n",
      "2.4900083541870117\n",
      "2.489286184310913\n",
      "2.4885904788970947\n",
      "2.487919569015503\n",
      "2.487272024154663\n",
      "2.4866466522216797\n",
      "2.4860432147979736\n",
      "2.485459327697754\n",
      "2.4848949909210205\n",
      "2.484349012374878\n",
      "2.4838204383850098\n",
      "2.4833083152770996\n",
      "2.48281192779541\n",
      "2.4823310375213623\n",
      "2.4818642139434814\n",
      "2.4814109802246094\n",
      "2.480971336364746\n",
      "2.4805445671081543\n",
      "2.4801294803619385\n",
      "2.4797260761260986\n",
      "2.4793341159820557\n",
      "2.478952169418335\n",
      "2.478581190109253\n",
      "2.478219985961914\n",
      "2.4778683185577393\n",
      "2.4775257110595703\n",
      "2.477191925048828\n",
      "2.4768667221069336\n",
      "2.4765498638153076\n",
      "2.476240873336792\n",
      "2.4759392738342285\n",
      "2.4756453037261963\n",
      "2.475358247756958\n",
      "2.4750781059265137\n",
      "2.474804401397705\n",
      "2.4745376110076904\n",
      "2.4742767810821533\n",
      "2.474022150039673\n",
      "2.4737727642059326\n",
      "2.473529577255249\n",
      "2.4732918739318848\n",
      "2.4730591773986816\n",
      "2.4728317260742188\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # Input to the network: one-hot encoding \n",
    "    logits = xenc @ W # predict log-counts \n",
    "    counts = logits.exp() # counts, equivalent to N \n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character \n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None \n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5462e+00,  2.1190e+00,  9.0085e-01,  1.0672e+00,  1.1590e+00,\n",
       "          1.0601e+00, -2.4535e-01,  2.3012e-01,  4.9830e-01,  1.0563e-01,\n",
       "          1.5193e+00,  1.7210e+00,  1.0865e+00,  1.5661e+00,  7.6991e-01,\n",
       "         -3.0225e-01, -3.2712e-02, -1.7625e+00,  1.1283e+00,  1.3548e+00,\n",
       "          9.0238e-01, -1.7914e+00, -3.4989e-01, -5.5373e-01, -1.3608e+00,\n",
       "          5.5917e-03,  5.5950e-01],\n",
       "        [ 2.2273e+00, -2.5596e-01, -2.8342e-01, -4.2487e-01,  3.7396e-01,\n",
       "         -3.6321e-02, -1.6577e+00, -1.4596e+00,  1.1804e+00,  8.3421e-01,\n",
       "         -1.4115e+00, -2.3451e-01,  1.2612e+00,  8.2446e-01,  2.0275e+00,\n",
       "         -2.2382e+00, -2.0526e+00, -2.3270e+00,  1.5169e+00,  4.4447e-01,\n",
       "         -4.3596e-02, -6.3626e-01,  1.5083e-01, -1.4932e+00, -1.3742e+00,\n",
       "          1.0515e+00, -5.0278e-01],\n",
       "        [ 8.4188e-01,  2.0740e+00, -3.0394e-02, -5.5325e-01, -1.3936e-01,\n",
       "          2.8412e+00, -7.0106e-01, -8.6571e-01, -6.1971e-01,  1.4827e+00,\n",
       "         -4.1527e-01, -1.2691e+00,  8.8199e-01, -2.3992e-01, -1.1758e+00,\n",
       "          8.0758e-01, -5.2107e-01, -3.1378e-01,  3.0994e+00, -9.7948e-01,\n",
       "         -1.1268e+00,  2.8879e-01, -6.8389e-01, -4.1647e-01, -1.1031e+00,\n",
       "          4.8133e-01, -1.4683e+00],\n",
       "        [ 1.7511e-01,  2.3701e+00, -1.8028e+00, -3.7723e-01, -1.0473e+00,\n",
       "          1.9683e+00, -1.5103e+00, -1.6994e+00,  2.1604e+00,  1.1929e+00,\n",
       "         -1.0637e+00,  1.3834e+00, -5.0101e-02, -1.4421e+00, -9.1381e-01,\n",
       "          1.5789e+00, -1.2395e+00, -1.6059e+00, -1.2346e-01, -1.5070e+00,\n",
       "         -9.2228e-01, -1.6578e+00, -1.2414e+00, -2.0427e+00, -1.2230e+00,\n",
       "         -3.2653e-01, -1.4294e+00],\n",
       "        [ 1.5606e+00,  2.5130e+00, -1.6209e+00, -1.3388e+00,  1.8378e-01,\n",
       "          2.4973e+00, -1.6078e+00, -1.1840e+00, -5.0819e-01,  1.8387e+00,\n",
       "         -1.2160e+00, -1.2711e+00, -7.7339e-01, -1.0812e+00, -1.0678e+00,\n",
       "          1.2251e+00, -1.7200e+00, -1.3581e+00,  1.3523e+00, -1.1815e+00,\n",
       "         -1.3220e+00, -2.6751e-01, -1.3031e+00, -1.2155e+00, -2.2103e+00,\n",
       "          1.0284e+00, -1.3901e+00],\n",
       "        [ 2.7688e+00,  9.9699e-01, -7.2082e-01, -4.9565e-01,  4.2345e-01,\n",
       "          1.6255e+00, -1.1041e+00, -6.8610e-01, -5.3124e-01,  1.1838e+00,\n",
       "         -1.2629e+00, -3.5550e-01,  2.5647e+00,  1.1219e+00,  2.3705e+00,\n",
       "          6.2567e-02, -1.0244e+00, -2.1004e+00,  2.0582e+00,  1.2352e+00,\n",
       "          8.3873e-01, -1.1515e+00,  6.1210e-01, -1.7629e+00, -6.6343e-01,\n",
       "          1.4530e+00, -3.3415e-01],\n",
       "        [ 1.6107e+00,  2.8160e+00,  9.3099e-01, -5.5178e-01,  1.8974e-01,\n",
       "          1.8821e+00,  4.0361e-01, -8.2895e-01,  6.7876e-01,  2.3127e+00,\n",
       "          3.4247e-01, -3.2625e-02, -1.1932e-01, -5.3227e-01, -9.2529e-01,\n",
       "          8.6928e-01,  7.1559e-01, -2.4494e-01,  1.9821e+00, -6.7217e-01,\n",
       "         -9.6066e-01,  3.5361e-01, -5.9627e-01, -1.9866e-01,  1.0942e-01,\n",
       "          5.2274e-01,  1.8211e-01],\n",
       "        [ 3.6312e-01,  2.0712e+00, -4.7452e-01, -5.8911e-01,  3.1668e-02,\n",
       "          2.0843e+00, -1.1733e+00, -2.2778e-01,  2.1613e+00,  1.3960e+00,\n",
       "         -9.4739e-01, -3.1299e-01,  1.3329e-01, -2.6037e-01, -4.7383e-03,\n",
       "         -1.9445e-02, -4.4527e-01, -4.2281e-01,  1.4442e+00, -6.5427e-02,\n",
       "         -1.0792e+00,  4.2255e-01, -3.8900e-01, -3.4987e-01, -1.3006e+00,\n",
       "         -4.6288e-01, -1.1348e+00],\n",
       "        [ 3.4412e+00,  3.3698e+00, -8.4929e-01, -1.0750e+00, -6.3453e-01,\n",
       "          2.1515e+00, -1.4927e+00, -1.1096e+00, -1.4510e+00,  2.2317e+00,\n",
       "         -1.5937e+00, -5.8221e-01,  6.1257e-01,  1.9141e-01,  3.0209e-01,\n",
       "          1.2565e+00, -1.2689e+00, -9.5149e-01,  8.5081e-01, -6.1782e-01,\n",
       "         -1.5293e-01,  6.7830e-01, -7.2694e-01, -1.0388e+00, -1.0660e+00,\n",
       "          9.3578e-01, -1.6117e+00],\n",
       "        [ 2.0195e+00,  2.0016e+00, -1.1054e+00,  4.2508e-01,  2.7761e-01,\n",
       "          1.6094e+00, -1.1840e+00,  2.4934e-01, -1.2154e+00, -1.3276e+00,\n",
       "         -1.4076e+00,  2.8901e-01,  1.4026e+00,  2.4689e-01,  1.8616e+00,\n",
       "          5.7078e-01, -1.5730e+00, -1.5832e+00,  9.4066e-01,  1.3808e+00,\n",
       "          4.8669e-01, -1.0982e+00, -2.2408e-01, -2.2763e+00, -1.2787e+00,\n",
       "          8.5413e-01, -1.9663e-01],\n",
       "        [ 4.9487e-01,  3.9698e+00, -3.5120e-01, -1.1043e+00, -8.3388e-01,\n",
       "          2.7180e+00, -1.2735e+00, -7.7533e-01, -8.0891e-01,  1.1784e+00,\n",
       "         -6.8198e-01, -6.1210e-01, -8.9622e-01, -1.8821e-01, -1.1193e+00,\n",
       "          2.8099e+00, -1.1873e+00, -3.0373e-01, -1.7669e+00, -2.3035e-01,\n",
       "         -1.2684e+00,  1.8150e+00, -6.5940e-01, -3.6645e-01, -1.2691e+00,\n",
       "         -5.8087e-01, -8.2898e-01],\n",
       "        [ 1.5952e+00,  3.2202e+00, -2.0515e+00, -1.6799e+00, -9.3624e-01,\n",
       "          2.5485e+00, -9.8769e-01, -9.8762e-01,  1.4171e+00,  1.9625e+00,\n",
       "         -1.8814e+00, -7.2385e-01,  5.4472e-01, -1.1684e+00, -1.0330e+00,\n",
       "          1.5424e+00, -1.1374e+00, -1.3192e+00,  2.3522e-01,  1.0240e-01,\n",
       "         -7.6078e-01, -6.1906e-01, -9.7435e-01, -6.4330e-01, -1.0920e+00,\n",
       "          1.6355e+00, -1.3279e+00],\n",
       "        [ 2.8211e+00,  3.5167e+00, -4.2079e-01, -1.2625e+00,  4.1473e-01,\n",
       "          3.6247e+00, -1.0839e+00, -1.0766e+00, -8.6798e-01,  3.4604e+00,\n",
       "         -1.7409e+00, -1.1803e+00,  2.8446e+00, -3.9571e-01, -9.2813e-01,\n",
       "          2.1711e+00, -8.1636e-01, -1.0211e+00, -1.1757e+00, -6.9248e-02,\n",
       "         -1.8042e-01,  1.3829e+00, -2.1083e-01, -8.4310e-01, -1.1877e+00,\n",
       "          3.0120e+00, -1.1685e+00],\n",
       "        [ 2.1229e+00,  3.7627e+00,  4.4794e-01, -2.6770e-01, -5.0886e-01,\n",
       "          2.5968e+00, -1.0508e+00, -1.0855e+00, -7.0054e-01,  3.0326e+00,\n",
       "         -1.5485e+00, -8.0473e-01, -1.2706e+00,  8.6655e-01, -9.6542e-01,\n",
       "          1.9843e+00, -3.4471e-01, -1.3972e+00,  3.2587e-01, -1.1939e+00,\n",
       "         -1.0868e+00,  4.5518e-01, -1.2989e+00, -1.1275e+00, -7.7099e-01,\n",
       "          1.4977e+00, -1.1847e+00],\n",
       "        [ 3.7023e+00,  2.8798e+00, -1.5345e+00,  1.7688e-01,  1.4255e+00,\n",
       "          2.0913e+00, -1.8165e+00,  4.4226e-01, -1.4557e+00,  2.3315e+00,\n",
       "         -1.3141e+00, -1.0337e+00,  6.4701e-02, -1.4037e+00,  2.4319e+00,\n",
       "          1.0673e+00, -1.5670e+00, -1.8480e+00, -1.2240e+00,  4.6272e-01,\n",
       "          9.5070e-01, -6.4285e-01, -1.1816e+00, -1.5075e+00, -1.7905e+00,\n",
       "          1.0007e+00, -2.7177e-01],\n",
       "        [ 1.6478e+00, -1.0041e-01, -3.0749e-01, -3.4915e-01,  1.2781e-01,\n",
       "         -3.1043e-01, -1.0740e+00, -9.3346e-01,  3.7351e-03, -8.8953e-01,\n",
       "         -2.2012e+00, -7.2104e-01,  1.3233e+00,  4.4863e-01,  2.6868e+00,\n",
       "         -4.2214e-01, -5.2345e-01, -1.7160e+00,  1.8625e+00,  1.1161e+00,\n",
       "         -3.3382e-01,  5.0281e-01,  2.7661e-02, -5.9799e-01, -1.1878e+00,\n",
       "         -4.8471e-01, -8.4531e-01],\n",
       "        [-2.1729e-01,  1.8425e+00, -5.3881e-01, -2.0821e-01, -4.5455e-01,\n",
       "          1.6748e+00, -5.6210e-01, -2.6022e-01,  1.7838e+00,  1.4718e-01,\n",
       "         -5.1313e-01, -8.1157e-01, -4.6993e-01, -6.6089e-01, -2.5823e+00,\n",
       "          4.3885e-01,  9.9509e-02, -2.1363e+00,  1.1983e+00, -7.3131e-01,\n",
       "         -6.0419e-01, -1.0863e+00, -1.2670e+00, -1.2996e+00, -9.4821e-02,\n",
       "         -3.7172e-01, -1.2336e+00],\n",
       "        [ 1.2743e+00,  5.9277e-01,  4.3778e-01, -2.2457e+00, -1.2009e+00,\n",
       "         -4.8742e-01,  3.8020e-01, -1.1323e+00,  8.7227e-02, -1.8350e+00,\n",
       "         -3.8706e-01, -7.7004e-01,  6.7608e-01, -1.1560e+00, -1.3204e+00,\n",
       "          7.4471e-01,  1.1455e+00,  5.6798e-01,  1.1781e+00,  2.9706e-01,\n",
       "         -7.8111e-01,  3.0274e+00, -7.7614e-01,  7.4798e-01, -6.8858e-01,\n",
       "          1.2921e+00,  9.7204e-04],\n",
       "        [ 1.9540e+00,  2.4925e+00, -1.3041e+00, -7.4543e-01, -7.0063e-02,\n",
       "          2.1636e+00, -1.6782e+00, -1.0665e+00, -4.7879e-01,  2.7456e+00,\n",
       "         -1.6865e+00, -1.1608e+00,  7.3992e-01, -2.0867e-01, -3.7622e-01,\n",
       "          1.4915e+00, -1.5956e+00, -1.5698e+00,  7.6882e-01, -5.6681e-02,\n",
       "          2.5377e-02,  2.1670e-01, -1.5643e+00, -1.5789e+00, -2.0752e+00,\n",
       "          1.3736e+00, -1.3825e+00],\n",
       "        [ 2.4938e+00,  2.5211e+00, -9.3878e-01, -4.2090e-01, -1.3742e+00,\n",
       "          2.2108e+00, -1.5027e+00, -1.4972e+00,  2.5894e+00,  1.9497e+00,\n",
       "         -1.6736e+00, -7.2644e-01,  1.0156e+00, -1.6761e-01, -1.1902e+00,\n",
       "          1.6902e+00, -5.5236e-01, -1.6631e+00, -6.1005e-01,  1.5436e+00,\n",
       "          2.0638e+00,  5.7133e-01, -1.3037e+00, -9.8642e-01, -1.2002e+00,\n",
       "          7.2993e-01, -9.5262e-01],\n",
       "        [ 2.1896e+00,  2.9689e+00, -1.0527e+00, -6.7693e-01, -6.3134e-01,\n",
       "          2.5992e+00, -1.1932e+00, -5.5991e-01,  2.4945e+00,  2.2909e+00,\n",
       "         -7.3317e-01, -6.1933e-01,  5.6858e-01, -8.1669e-01, -6.7500e-01,\n",
       "          2.5260e+00, -6.0805e-01, -9.3197e-01,  1.8510e+00, -2.1034e-01,\n",
       "          1.9175e+00,  1.5449e-01, -5.9872e-01, -5.8629e-01, -6.6082e-01,\n",
       "          1.8111e+00,  3.5699e-01],\n",
       "        [ 6.1411e-01,  6.0987e-01, -5.6896e-01,  2.9081e-01,  4.1158e-01,\n",
       "          6.4828e-01, -4.8291e-01, -4.2772e-01, -3.9676e-01,  3.9774e-01,\n",
       "         -9.0023e-01, -2.1341e-01,  1.2857e+00,  6.0405e-01,  1.2003e+00,\n",
       "         -8.0884e-01, -1.0158e+00, -1.1572e+00,  1.6224e+00,  1.7608e+00,\n",
       "         -2.1880e-02, -1.3254e+00, -8.7649e-01,  3.3274e-02, -7.2542e-01,\n",
       "         -5.7560e-01, -2.4690e-01],\n",
       "        [ 7.9689e-01,  3.1085e+00, -2.5208e-01, -1.2599e+00, -8.6799e-01,\n",
       "          2.9796e+00, -4.5207e-01, -1.0213e+00, -1.1512e+00,  3.4712e+00,\n",
       "         -1.3292e+00, -4.7165e-01, -8.8132e-01, -1.9233e-01, -1.0761e+00,\n",
       "          1.4192e+00, -3.7638e-01, -5.3565e-01, -5.2250e-01, -4.7062e-01,\n",
       "         -1.3392e+00, -7.9127e-01, -9.8705e-01, -7.4875e-01, -8.8304e-01,\n",
       "          1.0117e+00, -1.5179e+00],\n",
       "        [ 1.2429e+00,  2.7657e+00, -1.4551e+00,  2.1206e-01,  4.1137e-01,\n",
       "          1.9336e+00, -1.0815e-01,  2.1145e-01, -5.4576e-01,  1.9326e+00,\n",
       "         -6.3925e-01, -4.1484e-01, -4.6667e-02,  4.3286e-01,  1.3785e-01,\n",
       "         -3.2943e-01,  3.2112e-01, -8.1152e-01, -1.7288e-02, -5.5352e-01,\n",
       "          6.9569e-01,  4.6363e-01, -1.0698e-01, -1.1247e+00,  3.1050e-01,\n",
       "          1.1167e+00,  7.8310e-02],\n",
       "        [ 2.0755e+00,  9.6338e-01, -5.5990e-01, -3.5934e-02, -1.6015e+00,\n",
       "          8.5518e-01, -4.9077e-02, -2.5100e-01, -9.0315e-02,  1.3660e+00,\n",
       "          4.5162e-01, -9.9667e-01,  2.1350e-01, -1.0262e+00, -9.9873e-01,\n",
       "         -3.7335e-01, -4.6746e-01,  1.2408e-01, -2.0549e-01,  7.9279e-01,\n",
       "          1.4320e+00, -3.6087e-01, -2.1830e-01,  4.6641e-01,  1.8640e-01,\n",
       "         -3.2785e-01,  4.9082e-01],\n",
       "        [ 3.0170e+00,  3.0828e+00, -1.0338e+00,  6.3299e-02,  9.8583e-01,\n",
       "          1.0868e+00, -1.0409e+00, -6.6060e-01, -7.9051e-01,  5.8254e-01,\n",
       "         -1.1371e+00, -3.0537e-01,  2.4158e+00,  2.4197e-01,  2.9220e+00,\n",
       "          9.7485e-01, -9.0680e-01, -1.4500e+00,  1.0531e+00,  1.3869e+00,\n",
       "          4.7886e-02,  3.0769e-01, -1.4039e-01, -9.9057e-01, -9.2992e-01,\n",
       "         -7.8788e-01, -3.3566e-01],\n",
       "        [ 1.5942e+00,  3.4064e+00, -1.0176e+00, -1.7987e+00, -6.5709e-01,\n",
       "          2.5364e+00, -1.0036e+00, -4.5791e-01,  1.0960e-01,  2.5144e+00,\n",
       "         -9.6387e-01, -5.7349e-01,  9.7401e-01,  4.4317e-02, -5.7422e-01,\n",
       "          1.1168e+00, -3.0369e-01, -4.9795e-01,  2.5662e-01, -2.9136e-01,\n",
       "         -2.0145e-01,  6.7451e-01, -4.0349e-01,  6.9924e-02, -8.3950e-02,\n",
       "          1.3536e+00,  4.1859e-01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can.\n",
      "ahior.\n",
      "slea.\n",
      "eman.\n",
      "areiakialaveiphali.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647+1)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- EXERCISES -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e m\n",
      "e m m\n",
      "m m a\n",
      "m a .\n"
     ]
    }
   ],
   "source": [
    "# E01\n",
    "\n",
    "x1, x2, ys = [], [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    print(ch1, ch2, ch3)\n",
    "    x1.append(ix1)\n",
    "    x2.append(ix2)\n",
    "    ys.append(ix3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 13, 13]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 13, 13, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 13, 1, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(x1)\n",
    "x2 = torch.tensor(x2)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the \"network\"\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # Input to the network: one-hot encoding \n",
    "    logits = xenc @ W # predict log-counts \n",
    "    counts = logits.exp() # counts, equivalent to N \n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character \n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None \n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
